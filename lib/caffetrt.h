/*
 * Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef CAFFE_TRT
#define CAFFE_TRT

#include "argsParser.h"
#include "buffers.h"
#include "common.h"
#include "logger.h"

#include "NvCaffeParser.h"
#include "NvInfer.h"
#include <cuda_runtime_api.h>

#include <opencv2/core.hpp>
#include <opencv2/imgproc/imgproc.hpp>
#include <opencv2/imgcodecs.hpp>

// #include <opencv2/core/cuda.hpp>
// // #include <opencv2/cudaarithm.hpp>
// #include <opencv2/cudaimgproc.hpp>
// #include <opencv2/cudawarping.hpp>

#include <cstdlib>
#include <fstream>
#include <iostream>
#include <sstream>
#include <chrono>

// using GpuMat = cv::cuda::GpuMat;

namespace trt 
{

template<typename TimeT = std::chrono::microseconds, 
  typename ClockT=std::chrono::high_resolution_clock,
  typename DurationT=double>
class Stopwatch {
private:
  std::chrono::time_point<ClockT> _start, _end;
public:
  Stopwatch() { start(); }
  void start() { _start = _end = ClockT::now(); }
  DurationT stop() { _end = ClockT::now(); return elapsed();}
  DurationT elapsed() { 
      auto delta = std::chrono::duration_cast<TimeT>(_end-_start);
      return delta.count(); 
  }
};

//!
//! \brief Generic exception generated by this and derived classes
//!
class CaffeRuntimeException : public exception
{
public:
  CaffeRuntimeException(string m="") : msg(m) {}
  ~CaffeRuntimeException() throw() {}
  const char* what() const throw() { return msg.c_str(); }

private:
  string msg;
};

//!
//! \brief The CaffeNetworkParams structure groups the additional parameters required by
//!         the FasterRCNN sample.
//!
struct CaffeNetworkParams : public samplesCommon::CaffeSampleParams
{
  int outputClsSize;                      //!< The number of output classes
  std::string serializedWeightsFileName;  //!< Filename + Location of serialized engine
  std::string classesFileName;            //!< Classnames file to be detected
  uint preprocessDeadline      = 20000;   //!< Max time image can be resized
  unsigned int inferLoopLimit  = 60;      //!< Will wait for 60, 500us loops (30ms)
};

//!
//! \brief The Faster RCNN results to include label and bounding box
//!
struct DetectionObject
{
  samplesCommon::BBox box;    //!< Array of bounding boxes
  std::string label;          //!< classname of object
  float confidence;           //!< confidence level of detection
};

//! \brief  The Caffe class implements the FasterRCNN sample
//!
//! \details It creates the network using a caffe model
//!
template<int TBatchSize>
class Caffe
{
public: 
  //!
  //! \brief Function builds the network engine
  //!
  virtual bool initEngine();

  //!
  //! \brief Runs the TensorRT inference engine for this sample
  //!
  virtual std::array<std::vector<DetectionObject>, TBatchSize> infer(
    const std::array<cv::Mat, TBatchSize>& images) = 0;

  //!
  //! \brief Cleans up any state created in the sample class
  //!
  bool teardown();

  virtual ~Caffe() {
    gLogWarning << "Shutting down protobuf." << '\n';
    teardown();

    gLogWarning << "Shut down protobuf." << '\n';
  }

protected:
  template <typename T>
  using SampleUniquePtr = std::unique_ptr<T, samplesCommon::InferDeleter>;

  size_t mNbInputs;

  // std::array<float, TBatchSize> mScales;

  cv::Scalar mNetImageMean; //!< mean used by Network author

  CaffeNetworkParams mParams; //!< The parameters for the sample.

  nvinfer1::Dims mInputDims; //!< The dimensions of the input to the network.

  std::vector<std::string> mClasses; //!< Objects to be detected

  std::shared_ptr<nvinfer1::ICudaEngine> mEngine; //!< The TensorRT engine used to run the network

  //!
  //! \brief Preprocess image
  //!
  virtual void processInput(
    const samplesCommon::BufferManager& buffers, const std::array<cv::Mat, TBatchSize>& images);

private:
  //!
  //! \brief Load engine from file and create runtime
  //!
  bool loadEngine();

  //!
  //! \brief Creates the network, configures the builder and creates the network engine
  //!
  bool build();

  //!
  //! \brief Parses a Caffe model for FasterRCNN and creates a TensorRT network
  //!
  void constructNetwork(
    SampleUniquePtr<nvcaffeparser1::ICaffeParser>& parser,
    SampleUniquePtr<nvinfer1::IBuilder>& builder, 
    SampleUniquePtr<nvinfer1::INetworkDefinition>& network,
    SampleUniquePtr<nvinfer1::IBuilderConfig>& config);

  //!
  //! \brief Writes serialized model to file
  //!
  bool saveEngineToFile(void);
};

//!
//! \brief Creates the network, configures the builder and creates the network engine
//!
//! \details This function creates the FasterRCNN network by parsing the caffe model and builds
//!          the engine that will be used to run FasterRCNN (mEngine)
//!
//! \return Returns true if the engine was created successfully and false otherwise
//!
template<int TBatchSize>
inline
bool Caffe<TBatchSize>::build() {
  auto builder = SampleUniquePtr<nvinfer1::IBuilder>(nvinfer1::createInferBuilder(gLogger.getTRTLogger()));
  if (!builder)
    return false;

  auto network = SampleUniquePtr<nvinfer1::INetworkDefinition>(builder->createNetworkV2(0U));
  if (!network)
    return false;

  auto config = SampleUniquePtr<nvinfer1::IBuilderConfig>(builder->createBuilderConfig());
  if (!config)
    return false;

  auto parser = SampleUniquePtr<nvcaffeparser1::ICaffeParser>(nvcaffeparser1::createCaffeParser());
  if (!parser)
    return false;

  constructNetwork(parser, builder, network, config);

  mEngine = std::shared_ptr<nvinfer1::ICudaEngine>(
    builder->buildEngineWithConfig(*network, *config), samplesCommon::InferDeleter());

  if (!mEngine)
    return false;

  assert(network->getNbInputs() == mNbInputs);
  mInputDims = network->getInput(0)->getDimensions();
  assert(mInputDims.nbDims == 3);

  return saveEngineToFile();
}

//!
//! \brief Loads engine from serialized file
//!
//! \details This function creates the FasterRCNN network by loading serialized model
//!          from file that will be used to run FasterRCNN (mEngine)
//!
//! \return Returns true if the engine was created successfully and false otherwise
//!
template<int TBatchSize>
inline
bool Caffe<TBatchSize>::loadEngine() {
  std::ifstream engineFile(mParams.serializedWeightsFileName, std::ios::binary);
  if (!engineFile) {
    gLogError << "Error opening engine file: " << mParams.serializedWeightsFileName << '\n';
    return false;
  }

  std::string engineData;

  engineFile >> std::noskipws;
  std::copy(std::istream_iterator<char>(engineFile), std::istream_iterator<char>(), std::back_inserter(engineData));
  
  engineFile.close();
  if (engineData.empty()) {
    gLogError << "Error loading engine file: " << mParams.serializedWeightsFileName << '\n';
    return false;
  }

  auto runtime = SampleUniquePtr<nvinfer1::IRuntime>(nvinfer1::createInferRuntime(gLogger.getTRTLogger()));
  if (!runtime)
    return false;

  mEngine = std::shared_ptr<nvinfer1::ICudaEngine>(
    runtime->deserializeCudaEngine(engineData.data(), engineData.size(), nullptr), samplesCommon::InferDeleter());

  if (!mEngine)
    return false;
  
  if (TBatchSize > mEngine->getMaxBatchSize())
  {
    gLogError << "Desired Batch Size[" << TBatchSize 
              << "] > Deserialized Engine's Batch Size[" 
              <<  mEngine->getMaxBatchSize() << "]" << '\n';
    return false;
  }

  mInputDims = mEngine->getBindingDimensions(0);
  assert(mInputDims.nbDims == 3);

  return true;
}

//!
//! \brief Inits Engine
//!
//! \details This function creates the FasterRCNN network by either parsing the caffe 
//!          model and builds the engine that will be used to run FasterRCNN (mEngine)
//!          or loads serialized engine
//!
//! \return Returns true if the engine was created successfully and false otherwise
//!
template<int TBatchSize>
inline 
bool Caffe<TBatchSize>::initEngine() {
  if (samplesCommon::fileExists(mParams.serializedWeightsFileName)) 
    return loadEngine();

  return build();
}

//!
//! \brief Uses a caffe parser to create the FasterRCNN network and marks the
//!        output layers
//!
//! \param network Pointer to the network that will be populated with the FasterRCNN network
//!
//! \param builder Pointer to the engine builder
//!
template<int TBatchSize>
inline 
void Caffe<TBatchSize>::constructNetwork(SampleUniquePtr<nvcaffeparser1::ICaffeParser>& parser,
  SampleUniquePtr<nvinfer1::IBuilder>& builder, SampleUniquePtr<nvinfer1::INetworkDefinition>& network,
  SampleUniquePtr<nvinfer1::IBuilderConfig>& config) {
  const nvcaffeparser1::IBlobNameToTensor* blobNameToTensor = 
    parser->parse(locateFile(mParams.prototxtFileName, mParams.dataDirs).c_str(),
      locateFile(mParams.weightsFileName, mParams.dataDirs).c_str(), *network, nvinfer1::DataType::kHALF);

  for (auto& s : mParams.outputTensorNames)
    network->markOutput(*blobNameToTensor->find(s.c_str()));

  builder->setMaxBatchSize(TBatchSize);

  config->setMaxWorkspaceSize(1_GiB);
  config->setFlag(BuilderFlag::kFP16);
  config->setFlag(BuilderFlag::kSTRICT_TYPES);
  // samplesCommon::enableDLA(builder.get(), config.get(), mParams.dlaCore);
}

//!
//! \brief Cleans up any state created in the sample class
//!
template<int TBatchSize>
inline 
bool Caffe<TBatchSize>::teardown() {
  //! Clean up the libprotobuf files as the parsing is complete
  //! \note It is not safe to use any other part of the protocol buffers library after
  //! ShutdownProtobufLibrary() has been called.
  nvcaffeparser1::shutdownProtobufLibrary();
  
  return true;
}

//! \brief Writes serialized model to file
//!
//! \details Serializes engine file that was built and saves it to specified location
//!
//! \return Returns true if the engine was successfully serialized and written to file
//!
template<int TBatchSize>
inline 
bool Caffe<TBatchSize>::saveEngineToFile(void) {
  std::ofstream engineFile(mParams.serializedWeightsFileName, std::ios::binary);
  if (!engineFile) {
    gLogError << "Cannot open engine file: " << mParams.serializedWeightsFileName << '\n';
    return false;
  }

  SampleUniquePtr<nvinfer1::IHostMemory> serializedEngine{mEngine->serialize()};

  if (serializedEngine == nullptr) {
    gLogError << "Engine serialization failed" << '\n';
    return false;
  }

  engineFile.write(static_cast<char*>(serializedEngine->data()), serializedEngine->size());
  engineFile.close();
  
  return !engineFile.fail();
}
//! \brief Processes input image/s
//!
//! \details Takes a batch of images and preprocesses them which includes
//!          resizing, converting to float and subtracting the image mean
//!
template<int TBatchSize>
inline 
void Caffe<TBatchSize>::processInput(
  const samplesCommon::BufferManager& buffers, const std::array<cv::Mat, TBatchSize>& images) {
    
  const int inputC = this->mInputDims.d[0];
  const int inputH = this->mInputDims.d[1];
  const int inputW = this->mInputDims.d[2];

  float* hostDataBuffer = static_cast<float*>(buffers.getHostBuffer("data"));

  for (int i = 0, volImg = inputC * inputH * inputW, volChl = inputH * inputW; i < TBatchSize; ++i) {
    cv::Mat floatImg;

    cv::resize(images[i], floatImg, cv::Size(inputW, inputH), 0, 0); //, cv::INTER_AREA);

    floatImg.convertTo(floatImg, CV_32FC3);

    floatImg -= this->mNetImageMean;

    cv::Mat bgr[3] = {
      cv::Mat(inputH, inputW, CV_32FC1, (void*)(hostDataBuffer + i * volImg + 0 * volChl)),
      cv::Mat(inputH, inputW, CV_32FC1, (void*)(hostDataBuffer + i * volImg + 1 * volChl)),
      cv::Mat(inputH, inputW, CV_32FC1, (void*)(hostDataBuffer + i * volImg + 2 * volChl))
    };

    cv::split(floatImg, bgr);
  }
}
}
#endif // CAFFE_TRT